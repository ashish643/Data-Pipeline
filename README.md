# Data-Pipeline

# ğŸ› ï¸ Data Engineering Pipeline: XML â†’ Kafka â†’ S3 â†’ EMR â†’ Redshift (Orchestrated with Airflow)

This project implements a production-grade data pipeline using AWS and open-source tools to ingest, validate, and load large XML datasets into Redshift for analytics.

---

## ğŸ“Œ Use Case

Ingest **100GB XML files** daily from an on-prem SFTP server, parse and transform them, stream the data into Kafka, store in S3, and load into Amazon Redshift via EMR â€” all orchestrated by Apache Airflow.

---

## ğŸ§± Architecture Overview

FTP/SFTP (XML)
â†“
Python Parser (XML â†’ CSV)
â†“
Kafka Producer (Amazon MSK)
â†“
Kafka S3 Sink Connector
â†“
Amazon S3 (Raw Layer)
â†“
Apache Airflow (MWAA)
â”œâ”€â”€ EMR Cluster Creation
â”œâ”€â”€ Spark Step 1: File Validation
â”œâ”€â”€ Spark Step 2: Content Validation
â”œâ”€â”€ Spark Step 3: Truncated Stage Load
â”œâ”€â”€ Spark Step 4: Main Load
â””â”€â”€ EMR Termination
â†“
Amazon Redshift (Staging â†’ Main Table)


---

## ğŸ”— Components Used

| Component            | Role                                                                 |
|----------------------|----------------------------------------------------------------------|
| **SFTP (on-prem)**   | Source of large daily XML files                                      |
| **Python Agent**     | Connects to SFTP, validates via XSD, parses XML to CSV               |
| **AWS Secrets Manager** | Securely stores SFTP/Kafka credentials                          |
| **Apache Kafka (MSK)** | Streaming platform to publish parsed data                        |
| **Kafka S3 Sink Connector** | Automatically writes Kafka messages to S3                    |
| **Amazon S3**        | Raw data storage layer                                               |
| **Apache Airflow (MWAA)** | DAG orchestrates file detection, EMR steps, and retry logic     |
| **Amazon EMR**       | Spark jobs for validation and Redshift loading                      |
| **Amazon Redshift**  | Final analytics database                                             |

---

## ğŸ§ª EMR Steps Explained

| Step                   | Description                                                                 |
|------------------------|-----------------------------------------------------------------------------|
| `file_validation.py`   | Validates schema, structure, delimiter, header rows                        |
| `content_validation.py`| Checks data types, business rules, null handling                           |
| `truncated_stage_load.py` | Truncates Redshift staging table and loads new data from S3            |
| `main_load.py`         | Merges staged data into production Redshift tables                         |

---

## ğŸ“œ Airflow DAG Logic

- **Sensor**: Waits for new file in `s3://your-bucket/raw/yyyy/mm/dd/`
- **Create EMR Cluster**: On file arrival
- **Add Steps**: All Spark jobs added to EMR
- **Watch Last Step**: Polls for success/failure
- **Terminate EMR**: Whether pass/fail (with max 3 retries)

---

## ğŸ” Security

- **AWS Secrets Manager**: Stores SFTP/Kafka credentials securely
- **IAM Roles**: Used for accessing S3, EMR, and Redshift
- **VPC/Subnet isolation** for data security

---

## ğŸ§° Future Enhancements

- Replace EMR with AWS Glue for managed Spark
- Add AWS Step Functions for modular step control
- Integrate with Amazon QuickSight or Tableau for dashboards
- Add CloudWatch monitoring for EMR and Airflow failures

---

## ğŸ“‚ Repository Structure

â”œâ”€â”€ dags/
â”‚ â””â”€â”€ emr_orchestrated_etl.py # Airflow DAG
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ file_validation.py # Spark validation script
â”‚ â”œâ”€â”€ content_validation.py
â”‚ â”œâ”€â”€ truncated_stage_load.py
â”‚ â””â”€â”€ main_load.py
â”œâ”€â”€ README.md



---

## âœ… Summary

This pipeline provides an end-to-end automated solution for processing XML data with full fault-tolerance, retries, monitoring, and transformation using scalable cloud-native services.

---
